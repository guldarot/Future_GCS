{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 4: Hyperparameter Tuning and Mini-Project\n",
    "\n",
    "**Week 6: Supervised Learning Algorithms**\n",
    "\n",
    "## Overview\n",
    "Welcome to Class 4! Today, we’ll wrap up Week 6 by learning about **hyperparameter tuning** using grid search and applying everything we’ve learned in a **mini-project**. You’ll build classifiers (logistic regression, KNN, decision trees) to predict Iris species, tune one model, and compare performance using evaluation metrics from Class 3. This is your chance to put it all together!\n",
    "\n",
    "## Objectives\n",
    "- Understand what hyperparameters are and why tuning matters.\n",
    "- Learn how to use grid search for hyperparameter optimization.\n",
    "- Complete a mini-project to build, evaluate, and compare classifiers.\n",
    "- Gain hands-on experience with an end-to-end machine learning workflow.\n",
    "\n",
    "## Agenda\n",
    "1. Introduction to hyperparameters\n",
    "2. Grid search for tuning\n",
    "3. Mini-project: Build and compare classifiers\n",
    "\n",
    "Let’s dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Hyperparameters\n",
    "\n",
    "**Hyperparameters** are settings you choose before training a model, unlike parameters (e.g., weights) the model learns. They control how the model behaves.\n",
    "\n",
    "**Examples**:\n",
    "- **KNN**: `n_neighbors` (number of neighbors, *k*).\n",
    "- **Decision Trees**: `max_depth` (how deep the tree can grow).\n",
    "- **Logistic Regression**: `C` (regularization strength).\n",
    "\n",
    "**Why tune?** The right hyperparameters improve performance. Bad choices can lead to overfitting (too complex) or underfitting (too simple).\n",
    "\n",
    "**Question**: What might happen if `max_depth` is too large in a decision tree? (Hint: Think about Class 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grid Search for Tuning\n",
    "\n",
    "**Grid search** is a method to systematically test combinations of hyperparameters to find the best ones.\n",
    "\n",
    "**How it works**:\n",
    "- Define a \"grid\" of possible values (e.g., `n_neighbors = [3, 5, 7]`).\n",
    "- Train and evaluate the model for each combination using cross-validation.\n",
    "- Pick the combination with the best performance (e.g., highest accuracy).\n",
    "\n",
    "**Example**: For KNN, test `n_neighbors = [1, 3, 5]` and `weights = ['uniform', 'distance']`.\n",
    "\n",
    "We’ll use scikit-learn’s `GridSearchCV` to automate this. Let’s try it later in the mini-project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mini-Project: Build and Compare Classifiers\n",
    "\n",
    "For the mini-project, you’ll build classifiers to predict **Iris species** (setosa, versicolor, virginica) using the full Iris dataset (3 classes, all 4 features). You’ll train logistic regression, KNN, and decision trees, evaluate them with metrics from Class 3, and tune one model with grid search.\n",
    "\n",
    "**Steps**:\n",
    "1. Load and prepare the Iris dataset.\n",
    "2. Train logistic regression, KNN, and decision tree models.\n",
    "3. Evaluate models using accuracy, precision, recall, and F1-score.\n",
    "4. Tune one model (e.g., KNN) with grid search.\n",
    "5. Compare results and pick the best model.\n",
    "\n",
    "Let’s start coding! Feel free to work in pairs or individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load Iris dataset (full: 3 classes)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the data\n",
    "print(\"Feature names:\", iris.feature_names)\n",
    "print(\"Target names:\", iris.target_names)\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did we do?**\n",
    "- Loaded the full Iris dataset (150 samples, 4 features: sepal length, sepal width, petal length, petal width).\n",
    "- Kept all 3 classes (setosa, versicolor, virginica).\n",
    "- Split into 80% training (120 samples) and 20% testing (30 samples).\n",
    "\n",
    "Let’s train the three models with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(random_state=42, max_iter=200)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "print(\"Models trained! Ready to evaluate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s evaluate all models using accuracy, precision, recall, and F1-score. Since this is multi-class, we’ll average metrics across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute and print metrics (multi-class)\n",
    "def print_metrics(y_true, y_pred, model_name):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Accuracy: {acc:.3f}\")\n",
    "    print(f\"  Precision: {prec:.3f}\")\n",
    "    print(f\"  Recall: {rec:.3f}\")\n",
    "    print(f\"  F1-Score: {f1:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Compute metrics for all models\n",
    "print_metrics(y_test, y_pred_lr, \"Logistic Regression\")\n",
    "print_metrics(y_test, y_pred_knn, \"KNN (k=3)\")\n",
    "print_metrics(y_test, y_pred_dt, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn!**\n",
    "- Which model performs best based on accuracy? F1-score?\n",
    "- Are precision and recall balanced? Why might they differ slightly?\n",
    "\n",
    "Let’s visualize confusion matrices to see where errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(y_test, y_pred_lr, \"Logistic Regression Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test, y_pred_knn, \"KNN (k=3) Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test, y_pred_dt, \"Decision Tree Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "- Look at the confusion matrices. Which classes are confused most often (e.g., versicolor vs. virginica)?\n",
    "- Which model has the fewest errors?\n",
    "\n",
    "Now, let’s tune the KNN model using grid search to see if we can improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for KNN\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Initialize KNN\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Set up grid search\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Run grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_:.3f)\n",
    "\n",
    "# Evaluate tuned model on test set\n",
    "y_pred_knn_tuned = grid_search.predict(X_test)\n",
    "print_metrics(y_test, y_pred_knn_tuned, \"Tuned KNN\")\n",
    "plot_confusion_matrix(y_test, y_pred_knn_tuned, \"Tuned KNN Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn!**\n",
    "- Did the tuned KNN perform better than the default KNN (k=3)? Check accuracy and F1-score.\n",
    "- Look at the confusion matrix. Are there fewer errors?\n",
    "\n",
    "**Challenge**: Try tuning the decision tree instead! Copy the grid search code and use:\n",
    "```python\n",
    "param_grid = {'max_depth': [2, 3, 4, 5], 'min_samples_split': [2, 5, 10]}\n",
    "dt = DecisionTreeClassifier()\n",
    "```\n",
    "Run it and compare results (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "\n",
    "Today, you:\n",
    "- Learned about **hyperparameters** and **grid search**.\n",
    "- Completed a **mini-project** to predict Iris species.\n",
    "- Trained, evaluated, and tuned classifiers (logistic regression, KNN, decision trees).\n",
    "- Compared models using metrics and confusion matrices.\n",
    "\n",
    "**Deliverable**:\n",
    "- Share your findings: Which model performed best? Why? (Discuss in class or submit a short summary.)\n",
    "\n",
    "**Homework**:\n",
    "- Experiment with a different random seed (e.g., `random_state=123`) and see how results change.\n",
    "- Explore the [scikit-learn GridSearchCV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) (5-10 min).\n",
    "\n",
    "**What’s Next?**\n",
    "- Try other algorithms (e.g., SVM, random forests) or datasets (e.g., from Kaggle).\n",
    "- Dive deeper into feature engineering or real-world applications.\n",
    "\n",
    "Congratulations on completing Week 6! Any questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}