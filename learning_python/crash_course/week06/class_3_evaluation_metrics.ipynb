{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 3: Evaluation Metrics for Classification\n",
    "\n",
    "**Week 6: Supervised Learning Algorithms**\n",
    "\n",
    "## Overview\n",
    "Welcome to Class 3! Today, we'll learn how to **evaluate** classification models to understand their performance. We'll use **accuracy**, **precision**, **recall**, **F1-score**, and **confusion matrices** to assess the logistic regression, KNN, and decision tree models from Classes 1 and 2. By the end, you'll be able to measure and compare model quality using scikit-learn.\n",
    "\n",
    "## Objectives\n",
    "- Understand why evaluation metrics matter.\n",
    "- Learn the definitions and use cases for accuracy, precision, recall, and F1-score.\n",
    "- Explore confusion matrices to analyze model errors.\n",
    "- Evaluate and compare multiple models on the Iris dataset.\n",
    "\n",
    "## Agenda\n",
    "1. Why evaluate models?\n",
    "2. Understanding evaluation metrics\n",
    "3. Confusion matrix explained\n",
    "4. Hands-on: Evaluate models\n",
    "\n",
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Evaluate Models?\n",
    "\n",
    "Training a model isn’t enough—we need to know **how well it performs**. A model might seem good on training data but fail on new data. Evaluation metrics help us:\n",
    "- Quantify model performance.\n",
    "- Compare different models (e.g., logistic regression vs. KNN).\n",
    "- Identify issues like overfitting or poor predictions.\n",
    "\n",
    "**Example**: In Class 2, we saw predictions from KNN and decision trees. But how do we know which model is better?\n",
    "\n",
    "**Question**: Why might a model with 100% accuracy on training data still be bad? (Hint: Think about new data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Evaluation Metrics\n",
    "\n",
    "We’ll focus on four common metrics for classification:\n",
    "\n",
    "- **Accuracy**: Fraction of correct predictions (correct / total).\n",
    "  - Good for balanced datasets, but misleading if classes are imbalanced.\n",
    "  - Example: 90 correct out of 100 = 90% accuracy.\n",
    "\n",
    "- **Precision**: Fraction of positive predictions that are correct (true positives / predicted positives).\n",
    "  - Important when false positives are costly (e.g., spam detection).\n",
    "  - Example: If model predicts 10 emails as spam and 8 are actually spam, precision = 8/10 = 0.8.\n",
    "\n",
    "- **Recall**: Fraction of actual positives correctly identified (true positives / actual positives).\n",
    "  - Important when false negatives are costly (e.g., disease detection).\n",
    "  - Example: If 10 patients have a disease and model identifies 7, recall = 7/10 = 0.7.\n",
    "\n",
    "- **F1-Score**: Harmonic mean of precision and recall (2 * precision * recall / (precision + recall)).\n",
    "  - Balances precision and recall, useful for imbalanced data.\n",
    "  - Example: Precision = 0.8, recall = 0.7 → F1 = 2 * 0.8 * 0.7 / (0.8 + 0.7) ≈ 0.746.\n",
    "\n",
    "**Question**: When might precision matter more than recall? (Pause and discuss!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix Explained\n",
    "\n",
    "A **confusion matrix** summarizes a model’s predictions by comparing predicted vs. actual labels. For binary classification (e.g., setosa = 0, versicolor = 1), it looks like:\n",
    "\n",
    "|                  | Predicted 0 | Predicted 1 |\n",
    "|------------------|-------------|-------------|\n",
    "| **Actual 0**     | True Negative (TN) | False Positive (FP) |\n",
    "| **Actual 1**     | False Negative (FN) | True Positive (TP) |\n",
    "\n",
    "- **True Positive (TP)**: Correctly predicted positive (e.g., predicted versicolor, actually versicolor).\n",
    "- **True Negative (TN)**: Correctly predicted negative.\n",
    "- **False Positive (FP)**: Incorrectly predicted positive (e.g., predicted versicolor, actually setosa).\n",
    "- **False Negative (FN)**: Incorrectly predicted negative.\n",
    "\n",
    "Metrics are calculated from the matrix:\n",
    "- Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "- Precision = TP / (TP + FP)\n",
    "- Recall = TP / (TP + FN)\n",
    "\n",
    "We’ll visualize confusion matrices to make errors clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hands-On: Evaluate Models\n",
    "\n",
    "We’ll evaluate the logistic regression, KNN, and decision tree models from Classes 1 and 2 on the Iris dataset (binary: setosa vs. versicolor). We’ll compute accuracy, precision, recall, F1-score, and visualize confusion matrices.\n",
    "\n",
    "**Steps**:\n",
    "1. Load and prepare the Iris dataset.\n",
    "2. Train all three models.\n",
    "3. Compute evaluation metrics.\n",
    "4. Visualize confusion matrices.\n",
    "\n",
    "Let’s dive into the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create binary classification: setosa (0) vs. versicolor (1)\n",
    "mask = y < 2\n",
    "X_binary = X[mask]\n",
    "y_binary = y[mask]\n",
    "\n",
    "# Use two features (petal length, petal width) for consistency\n",
    "X_binary = X_binary[:, 2:4]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binary, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the data\n",
    "print(\"Feature names:\", iris.feature_names[2:4])\n",
    "print(\"Target names:\", iris.target_names[:2])\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did we do?**\n",
    "- Loaded the same binary Iris dataset (100 samples, petal length/width).\n",
    "- Split into 80% training (80 samples) and 20% testing (20 samples).\n",
    "\n",
    "Now, let’s train all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "print(\"Models trained! Ready to evaluate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compute evaluation metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute and print metrics\n",
    "def print_metrics(y_true, y_pred, model_name):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Accuracy: {acc:.3f}\")\n",
    "    print(f\"  Precision: {prec:.3f}\")\n",
    "    print(f\"  Recall: {rec:.3f}\")\n",
    "    print(f\"  F1-Score: {f1:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Compute metrics for all models\n",
    "print_metrics(y_test, y_pred_lr, \"Logistic Regression\")\n",
    "print_metrics(y_test, y_pred_knn, \"KNN (k=3)\")\n",
    "print_metrics(y_test, y_pred_dt, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn!**\n",
    "- Look at the metrics. Which model has the highest accuracy? F1-score?\n",
    "- Are precision and recall similar for all models? Why might they differ?\n",
    "\n",
    "Now, let’s visualize the confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names[:2], yticklabels=iris.target_names[:2])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(y_test, y_pred_lr, \"Logistic Regression Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test, y_pred_knn, \"KNN (k=3) Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test, y_pred_dt, \"Decision Tree Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "- Look at the confusion matrices:\n",
    "  - **Diagonal** (top-left, bottom-right): Correct predictions (TN, TP).\n",
    "  - **Off-diagonal**: Errors (FP, FN).\n",
    "- Which model has the fewest errors?\n",
    "- Do any models make more false positives than false negatives?\n",
    "\n",
    "**Your turn!**\n",
    "- Try changing the KNN `n_neighbors` to 7 in the training cell and re-run the metrics and confusion matrix. Does performance improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "\n",
    "Today, you:\n",
    "- Learned why **evaluation metrics** are critical.\n",
    "- Computed **accuracy**, **precision**, **recall**, and **F1-score**.\n",
    "- Used **confusion matrices** to analyze model errors.\n",
    "- Evaluated logistic regression, KNN, and decision tree models.\n",
    "\n",
    "**Homework**:\n",
    "- Re-run the notebook with a different train-test split (change `random_state` to 123) and check how metrics change.\n",
    "- Explore the [scikit-learn metrics documentation](https://scikit-learn.org/stable/modules/model_evaluation.html) (5-10 min).\n",
    "\n",
    "**Next Class**:\n",
    "- We’ll cover **hyperparameter tuning** with grid search and work on a **mini-project** to tie everything together.\n",
    "- Be ready to build and compare models!\n",
    "\n",
    "Any questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}