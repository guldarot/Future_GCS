{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2: K-Nearest Neighbors and Decision Trees\n",
    "\n",
    "**Week 6: Supervised Learning Algorithms**\n",
    "\n",
    "## Overview\n",
    "Welcome to Class 2! Today, we'll explore two new supervised learning algorithms: **k-nearest neighbors (KNN)** and **decision trees**. We'll compare them to the logistic regression model from Class 1 and see how they work for classification tasks. By the end, you'll train KNN and decision tree models using scikit-learn on the Iris dataset.\n",
    "\n",
    "## Objectives\n",
    "- Understand how k-nearest neighbors (KNN) classifies data based on proximity.\n",
    "- Learn how decision trees make predictions using hierarchical rules.\n",
    "- Compare KNN, decision trees, and logistic regression.\n",
    "- Train and visualize predictions for both algorithms.\n",
    "\n",
    "## Agenda\n",
    "1. Introduction to k-nearest neighbors (KNN)\n",
    "2. Introduction to decision trees\n",
    "3. Hands-on: Train KNN and decision tree models\n",
    "4. Compare predictions\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to K-Nearest Neighbors (KNN)\n",
    "\n",
    "**K-nearest neighbors (KNN)** is a simple, intuitive classification algorithm. It predicts the class of a new sample by looking at the **k closest samples** in the training data and taking a majority vote.\n",
    "\n",
    "**How it works**:\n",
    "- Compute the distance (e.g., Euclidean) between a new sample and all training samples.\n",
    "- Find the *k* closest samples (neighbors).\n",
    "- Assign the class that appears most among those neighbors.\n",
    "\n",
    "**Key parameter**: *k* (number of neighbors).\n",
    "- Small *k* (e.g., 1): Sensitive to noise, overfitting.\n",
    "- Large *k*: Smoother predictions, but may miss patterns.\n",
    "\n",
    "**Example**: If a new Iris flower's closest 3 neighbors are two *setosa* and one *versicolor*, KNN predicts *setosa*.\n",
    "\n",
    "**Pros**: Simple, no training phase. **Cons**: Slow for large datasets, sensitive to irrelevant features.\n",
    "\n",
    "**Question**: What might happen if *k* is too large? (Pause and discuss!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Decision Trees\n",
    "\n",
    "**Decision trees** are classification algorithms that split data into regions based on feature values, creating a tree of decisions.\n",
    "\n",
    "**How it works**:\n",
    "- Start at the root and ask a question (e.g., \"Is petal length > 2.5 cm?\").\n",
    "- Follow branches based on answers, splitting data at each node.\n",
    "- Reach a leaf node, which gives the predicted class.\n",
    "\n",
    "**Key parameter**: Max depth (how many splits).\n",
    "- Shallow trees: Simple, may underfit.\n",
    "- Deep trees: Complex, may overfit.\n",
    "\n",
    "**Example**: A decision tree might split Iris data first by petal length, then petal width, to separate species.\n",
    "\n",
    "**Pros**: Interpretable, handles mixed data. **Cons**: Can overfit, sensitive to small changes.\n",
    "\n",
    "**Question**: Why might a very deep tree be a problem? (Hint: Think about memorizing the data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hands-On: Train KNN and Decision Tree Models\n",
    "\n",
    "We'll use the same Iris dataset (binary: *setosa* vs. *versicolor*) as Class 1 to train KNN and decision tree models. We'll also revisit logistic regression for comparison.\n",
    "\n",
    "**Steps**:\n",
    "1. Load and prepare the Iris dataset (same as Class 1).\n",
    "2. Train a KNN model and experiment with *k*.\n",
    "3. Train a decision tree model.\n",
    "4. Visualize predictions and decision boundaries.\n",
    "\n",
    "Let’s get coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create binary classification: setosa (0) vs. versicolor (1)\n",
    "mask = y < 2\n",
    "X_binary = X[mask]\n",
    "y_binary = y[mask]\n",
    "\n",
    "# Use two features (petal length, petal width) for visualization\n",
    "X_binary = X_binary[:, 2:4]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binary, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the data\n",
    "print(\"Feature names:\", iris.feature_names[2:4])\n",
    "print(\"Target names:\", iris.target_names[:2])\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did we do?**\n",
    "- Loaded the same binary Iris dataset as Class 1 (100 samples, petal length/width).\n",
    "- Split into 80% training (80 samples) and 20% testing (20 samples).\n",
    "\n",
    "Let’s train a KNN model with *k=3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "print(\"KNN (k=3) predictions:\", y_pred_knn)\n",
    "print(\"Actual labels:\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn!**\n",
    "- Compare `y_pred_knn` and `y_test`. How many predictions are correct?\n",
    "- Try changing `n_neighbors` to 1 or 7 in the cell below and re-run. What changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different k\n",
    "knn_experiment = KNeighborsClassifier(n_neighbors=7)  # Try 1, 5, 7, etc.\n",
    "knn_experiment.fit(X_train, y_train)\n",
    "y_pred_knn_experiment = knn_experiment.predict(X_test)\n",
    "print(\"KNN (new k) predictions:\", y_pred_knn_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s train a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train decision tree model\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "print(\"Decision tree predictions:\", y_pred_dt)\n",
    "print(\"Actual labels:\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Compare the decision tree predictions to KNN. Are they similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Predictions\n",
    "\n",
    "Let’s train a logistic regression model (from Class 1) and visualize decision boundaries for all three algorithms to see how they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression for comparison\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Function to plot decision boundaries\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"coolwarm\")\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Setosa\", color=\"blue\")\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Versicolor\", color=\"orange\")\n",
    "    plt.xlabel(\"Petal Length (cm)\")\n",
    "    plt.ylabel(\"Petal Width (cm)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "# Plot decision boundaries for all models\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plot_decision_boundary(lr, X_binary, y_binary, \"Logistic Regression\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_decision_boundary(knn, X_binary, y_binary, \"KNN (k=3)\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plot_decision_boundary(dt, X_binary, y_binary, \"Decision Tree\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "- Look at the decision boundaries:\n",
    "  - **Logistic Regression**: Linear boundary (straight line).\n",
    "  - **KNN**: Wavy, follows data points closely.\n",
    "  - **Decision Tree**: Blocky, rectangular splits.\n",
    "- Which boundary looks most flexible? Which looks simplest?\n",
    "- Try changing `n_neighbors` or `max_depth` and re-run the plots. What changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "\n",
    "Today, you:\n",
    "- Learned how **KNN** uses neighbors to classify data.\n",
    "- Explored **decision trees** and their rule-based splits.\n",
    "- Trained both models on the Iris dataset.\n",
    "- Compared decision boundaries with logistic regression.\n",
    "\n",
    "**Homework**:\n",
    "- Re-run the KNN model with a different *k* (e.g., 1, 10) and note how predictions change.\n",
    "- Check out the [scikit-learn KNN documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and [DecisionTreeClassifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) (5-10 min).\n",
    "\n",
    "**Next Class**:\n",
    "- We’ll dive into **evaluation metrics** (accuracy, precision, recall, F1-score) to measure how good our models are.\n",
    "- Bring questions about today’s models!\n",
    "\n",
    "Any questions before we finish?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}