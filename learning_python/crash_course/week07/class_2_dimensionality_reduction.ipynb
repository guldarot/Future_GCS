{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2: Dimensionality Reduction with PCA\n",
    "\n",
    "**Week 7: Unsupervised Learning and Advanced Data Analysis**\n",
    "\n",
    "**Objective**: Learn dimensionality reduction and apply Principal Component Analysis (PCA) to simplify datasets.\n",
    "\n",
    "**Agenda**:\n",
    "- Understand why dimensionality reduction is useful.\n",
    "- Explore PCA: how it works and what it reveals.\n",
    "- Demo: Apply PCA to a dataset and visualize results.\n",
    "- Exercise: Reduce dimensions of a dataset and interpret components.\n",
    "\n",
    "Let’s simplify complex data and uncover its structure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Dimensionality Reduction?\n",
    "\n",
    "- **Problem**: High-dimensional data (many features) is hard to visualize, analyze, or model.\n",
    "  - **Curse of Dimensionality**: More features can lead to overfitting, noise, or computational challenges.\n",
    "- **Solution**: Reduce dimensions while preserving important information.\n",
    "- **Applications**:\n",
    "  - Visualizing high-dimensional data in 2D/3D.\n",
    "  - Speeding up machine learning models.\n",
    "  - Removing redundant or noisy features.\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a popular technique to achieve this by transforming data into a new set of features (principal components)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How PCA Works\n",
    "\n",
    "**Goal**: Find new axes (principal components) that capture the most variance in the data.\n",
    "\n",
    "**Intuition**:\n",
    "- Imagine data as a cloud of points in high-dimensional space.\n",
    "- PCA finds the directions (axes) where the cloud spreads out the most.\n",
    "- The first principal component (PC1) captures the most variance, PC2 the second most, and so on.\n",
    "\n",
    "**Steps**:\n",
    "1. Standardize the data (zero mean, unit variance).\n",
    "2. Compute the covariance matrix to understand feature relationships.\n",
    "3. Find eigenvectors (directions) and eigenvalues (amount of variance) of the covariance matrix.\n",
    "4. Project the data onto the top *k* eigenvectors (principal components).\n",
    "\n",
    "**Key Outputs**:\n",
    "- **Explained Variance Ratio**: How much variance each component captures.\n",
    "- **Scree Plot**: Visualizes the importance of components.\n",
    "- **Transformed Data**: Lower-dimensional representation.\n",
    "\n",
    "**Applications**:\n",
    "- Visualize customer data in 2D after reducing from many features.\n",
    "- Preprocess data for clustering (like k-means from Class 1).\n",
    "\n",
    "Let’s see PCA in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demo: PCA on the Iris Dataset\n",
    "\n",
    "We’ll use the Iris dataset (4 features: sepal length, sepal width, petal length, petal width) to demonstrate PCA, reducing it to 2D for visualization.\n",
    "\n",
    "**Setup**: Ensure you have the required libraries installed (same as Class 1):\n",
    "```bash\n",
    "pip install numpy pandas scikit-learn matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['Species'] = y\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Species', palette='Set1', data=pca_df, s=100, alpha=0.7)\n",
    "plt.title('PCA of Iris Dataset (2D Projection)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(iris.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance ratio\n",
    "print('Explained Variance Ratio:', pca.explained_variance_ratio_)\n",
    "print('Total Variance Explained:', sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Scree plot\n",
    "pca_full = PCA().fit(X_scaled)  # Fit PCA with all components\n",
    "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
    "         pca_full.explained_variance_ratio_, 'bo-')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xticks(range(1, len(pca_full.explained_variance_ratio_) + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "- **\n",
    "- The scatter plot shows how PCA separates Iris species in 2D.\n",
    "- PC1 captures the most variance, PC2 the next most.\n",
    "- The scree plot suggests 2–3 components explain most of the data’s structure.\n",
    "- How might this help with clustering (like k-means from Class 1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercise: Apply PCA to a Dataset\n",
    "\n",
    "Now it’s your turn! Apply PCA to a dataset and interpret the results.\n",
    "\n",
    "**Task**:\n",
    "- Use the Iris dataset (or synthetic data if you prefer).\n",
    "- Reduce it to 2 dimensions using PCA.\n",
    "- Visualize the results and check the explained variance.\n",
    "- Bonus: Examine the PCA components to see which features contribute most.\n",
    "\n",
    "**Instructions**:\n",
    "1. Run the code below to load and standardize the data.\n",
    "2. Apply PCA and plot the 2D projection.\n",
    "3. Check the explained variance and scree plot.\n",
    "4. (Optional) Interpret the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and standardize data (using Iris again for simplicity)\n",
    "X_ex = iris.data\n",
    "scaler_ex = StandardScaler()\n",
    "X_scaled_ex = scaler_ex.fit_transform(X_ex)\n",
    "\n",
    "# Your code: Apply PCA\n",
    "pca_ex = PCA(n_components=2)\n",
    "X_pca_ex = pca_ex.fit_transform(X_scaled_ex)\n",
    "\n",
    "# Create DataFrame\n",
    "pca_df_ex = pd.DataFrame(X_pca_ex, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_df_ex['PC1'], pca_df_ex['PC2'], c='blue', s=100, alpha=0.7)\n",
    "plt.title('Your PCA Projection')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: Check explained variance\n",
    "print('Explained Variance Ratio:', pca_ex.explained_variance_ratio_)\n",
    "\n",
    "# Scree plot\n",
    "pca_full_ex = PCA().fit(X_scaled_ex)\n",
    "plt.plot(range(1, len(pca_full_ex.explained_variance_ratio_) + 1), \n",
    "         pca_full_ex.explained_variance_ratio_, 'bo-')\n",
    "plt.title('Your Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xticks(range(1, len(pca_full_ex.explained_variance_ratio_) + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Inspect PCA components\n",
    "# Components show the contribution of each original feature\n",
    "components_df = pd.DataFrame(pca_ex.components_, columns=feature_names, index=['PC1', 'PC2'])\n",
    "print('PCA Components:\\n', components_df)\n",
    "\n",
    "# Visualize as a heatmap\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(components_df, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Contributions to Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wrap-Up\n",
    "\n",
    "**Key Takeaways**:\n",
    "- PCA reduces dimensions by finding directions of maximum variance.\n",
    "- Explained variance and scree plots help decide how many components to keep.\n",
    "- PCA is great for visualization and preprocessing (e.g., before clustering).\n",
    "\n",
    "**Discussion Questions**:\n",
    "- What patterns did you see in the 2D projection?\n",
    "- Which features contributed most to PC1 and PC2?\n",
    "- How could PCA help with the mall customer dataset?\n",
    "\n",
    "**Homework**:\n",
    "- Think about how PCA could simplify the mall customer dataset.\n",
    "- Explore its features (e.g., age, income, spending score) and hypothesize what PC1 might represent.\n",
    "- Bring ideas to Class 3!\n",
    "\n",
    "Awesome work! Next, we’ll explore data distributions and feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}