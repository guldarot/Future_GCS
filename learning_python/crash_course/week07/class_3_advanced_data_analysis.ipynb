{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 3: Exploring Data Distributions and Feature Selection\n",
    "\n",
    "**Week 7: Unsupervised Learning and Advanced Data Analysis**\n",
    "\n",
    "**Objective**: Learn to analyze data distributions and select meaningful features for better modeling.\n",
    "\n",
    "**Agenda**:\n",
    "- Explore data using histograms, box plots, and correlation analysis.\n",
    "- Understand feature selection techniques: removing low-variance features and correlation-based selection.\n",
    "- Demo: Analyze the mall customer dataset.\n",
    "- Exercise: Visualize distributions and select features.\n",
    "\n",
    "Let’s dive into understanding and refining our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring Data Distributions\n",
    "\n",
    "**Why Explore Data?**\n",
    "- Understand the shape, spread, and patterns in your data.\n",
    "- Identify outliers, skewness, or relationships between features.\n",
    "- Inform preprocessing steps (e.g., scaling for PCA or clustering).\n",
    "\n",
    "**Tools**:\n",
    "- **Histograms**: Show the distribution of a single feature (e.g., how spending varies).\n",
    "- **Box Plots**: Highlight median, quartiles, and outliers.\n",
    "- **Correlation Analysis**: Measure relationships between features (e.g., does age correlate with income?).\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Outliers**: Extreme values that may distort models.\n",
    "- **Correlation**: Pearson correlation ranges from -1 (negative) to 1 (positive). High correlation may indicate redundancy.\n",
    "- **Multicollinearity**: When features are highly correlated, they may add little unique information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection Techniques\n",
    "\n",
    "**Why Select Features?**\n",
    "- Reduce noise and improve model performance.\n",
    "- Avoid redundancy (e.g., highly correlated features).\n",
    "- Make models faster and easier to interpret.\n",
    "\n",
    "**Methods Covered**:\n",
    "- **Low-Variance Features**: Remove features with little variation (they don’t help distinguish data points).\n",
    "- **Correlation-Based Selection**: Drop one of two highly correlated features to reduce redundancy.\n",
    "\n",
    "**Applications**:\n",
    "- Prepare data for clustering (Class 1) or PCA (Class 2).\n",
    "- Simplify the mall customer dataset for our mini-project.\n",
    "\n",
    "Let’s apply these ideas to real data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demo: Analyzing the Mall Customer Dataset\n",
    "\n",
    "We’ll explore the mall customer dataset, visualize distributions, and select features to prepare for clustering.\n",
    "\n",
    "**Dataset**: Contains customer data with features like age, annual income, and spending score.\n",
    "\n",
    "**Setup**: Ensure libraries are installed:\n",
    "```bash\n",
    "pip install numpy pandas scikit-learn matplotlib seaborn\n",
    "```\n",
    "\n",
    "**Note**: Download `Mall_Customers.csv` from [Kaggle](https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python) or your course platform and place it in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Mall_Customers.csv')\n",
    "\n",
    "# Drop non-numeric column (if any) and rename for clarity\n",
    "data = data.drop(columns=['CustomerID'], errors='ignore')\n",
    "data = data.rename(columns={'Annual Income (k$)': 'Income', 'Spending Score (1-100)': 'Spending'})\n",
    "\n",
    "# Display first few rows\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, col in enumerate(['Age', 'Income', 'Spending'], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.histplot(data[col], bins=20, kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, col in enumerate(['Age', 'Income', 'Spending'], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.boxplot(y=data[col])\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "corr = data.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection: Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.01)  # Arbitrary threshold\n",
    "X = data[['Age', 'Income', 'Spending']]  # Numeric features\n",
    "selector.fit(X)\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print('Features after variance threshold:', selected_features.tolist())\n",
    "\n",
    "# Correlation-based selection (manual example)\n",
    "high_corr = 0.7\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > high_corr)]\n",
    "print('Features to drop due to high correlation:', to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "- **Histograms**: What do the distributions tell us? (e.g., Is spending skewed?)\n",
    "- **Box Plots**: Are there outliers in income or spending?\n",
    "- **Correlation**: Are any features strongly correlated? How might this affect clustering?\n",
    "- **Feature Selection**: Did we drop any features? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercise: Explore and Select Features\n",
    "\n",
    "Your turn! Analyze the mall customer dataset and select features.\n",
    "\n",
    "**Task**:\n",
    "- Create histograms and box plots for Age, Income, and Spending.\n",
    "- Generate a correlation heatmap.\n",
    "- Apply low-variance feature selection and check for highly correlated features.\n",
    "- Interpret what you find.\n",
    "\n",
    "**Instructions**:\n",
    "1. Use the code below to load the data.\n",
    "2. Create visualizations (histograms, box plots, heatmap).\n",
    "3. Perform feature selection.\n",
    "4. Answer: Which features would you keep for clustering? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (same as demo)\n",
    "data_ex = pd.read_csv('Mall_Customers.csv')\n",
    "data_ex = data_ex.drop(columns=['CustomerID'], errors='ignore')\n",
    "data_ex = data_ex.rename(columns={'Annual Income (k$)': 'Income', 'Spending Score (1-100)': 'Spending'})\n",
    "\n",
    "# Your code: Histograms\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, col in enumerate(['Age', 'Income', 'Spending'], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.histplot(data_ex[col], bins=20, kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: Box plots\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, col in enumerate(['Age', 'Income', 'Spending'], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.boxplot(y=data_ex[col])\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: Correlation heatmap\n",
    "corr_ex = data_ex.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_ex, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Your Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: Feature selection\n",
    "# Low-variance\n",
    "selector_ex = VarianceThreshold(threshold=0.01)\n",
    "X_ex = data_ex[['Age', 'Income', 'Spending']]\n",
    "selector_ex.fit(X_ex)\n",
    "selected_features_ex = X_ex.columns[selector_ex.get_support()]\n",
    "print('Selected features (variance):', selected_features_ex.tolist())\n",
    "\n",
    "# Correlation-based\n",
    "corr_matrix_ex = X_ex.corr().abs()\n",
    "upper_ex = corr_matrix_ex.where(np.triu(np.ones(corr_matrix_ex.shape), k=1).astype(bool))\n",
    "to_drop_ex = [column for column in upper_ex.columns if any(upper_ex[column] > 0.7)]\n",
    "print('Features to drop (correlation):', to_drop_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation**:\n",
    "- What do the histograms and box plots reveal?\n",
    "- Are there strong correlations? Should we drop any features?\n",
    "- Which features would you keep for clustering? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wrap-Up\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Histograms and box plots reveal distributions and outliers.\n",
    "- Correlation analysis identifies relationships and redundancy.\n",
    "- Feature selection (low-variance, correlation-based) improves data quality.\n",
    "\n",
    "**Discussion Questions**:\n",
    "- What surprised you about the data’s distributions?\n",
    "- How might outliers affect k-means or PCA?\n",
    "- Which features seem most important for customer segmentation?\n",
    "\n",
    "**Homework**:\n",
    "- Apply feature selection to the mall customer dataset.\n",
    "- Prepare a clean dataset (selected features) for clustering in Class 4.\n",
    "- Think about how these features might form clusters.\n",
    "\n",
    "Great job exploring data! Next, we’ll tie it all together with clustering and cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}