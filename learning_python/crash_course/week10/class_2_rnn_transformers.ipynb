{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2: Recurrent Neural Networks (RNNs) and Transformers Overview\n",
    "\n",
    "## Welcome to Week 10, Class 2!\n",
    "In this notebook, weâ€™ll explore **sequence models**, which are crucial for tasks like sentiment analysis and text generation where word order matters. Weâ€™ll cover **Recurrent Neural Networks (RNNs)** and introduce **transformers**, then apply them to sentiment analysis, building on Class 1â€™s NLP skills.\n",
    "\n",
    "**Objectives**:\n",
    "- Understand why sequence models are needed for text.\n",
    "- Learn the basics of RNNs and their challenges.\n",
    "- Get a high-level overview of transformers and attention.\n",
    "- Train sentiment analysis models using an RNN (LSTM) and a transformer.\n",
    "\n",
    "**Letâ€™s dive in!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Sequence Models?\n",
    "In Class 1, we used **bag-of-words (BoW)**, which ignores word order (e.g., \"not good\" vs. \"good\"). Sequence models like RNNs and transformers:\n",
    "- Capture **context** and **order** in text.\n",
    "- Handle tasks like:\n",
    "  - Sentiment analysis (e.g., understanding long reviews).\n",
    "  - Text generation (e.g., writing sentences).\n",
    "  - Translation (e.g., English to Spanish).\n",
    "\n",
    "**Discussion Question**: Why does word order matter in sentences like \"The movie is great\" vs. \"Is the movie great\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "Letâ€™s install and import the required libraries. Run the cell below to set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (uncomment if needed)\n",
    "# !pip install numpy pandas tensorflow scikit-learn transformers datasets torch\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recurrent Neural Networks (RNNs)\n",
    "RNNs process sequences (e.g., words in a sentence) one step at a time, maintaining a **hidden state** to remember context.\n",
    "\n",
    "**Key Points**:\n",
    "- **Architecture**: Input â†’ Hidden State â†’ Output, looped over time steps.\n",
    "- **Use Case**: Sentiment analysis, where earlier words affect later ones.\n",
    "- **Challenge**: **Vanishing gradients** make it hard to learn long-term dependencies (e.g., context from 50 words ago).\n",
    "- **Solution**: Variants like **LSTM** (Long Short-Term Memory) improve memory.\n",
    "\n",
    "**Analogy**: Reading a book and remembering key plot points as you go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Hands-On: Sentiment Analysis with LSTM\n",
    "Letâ€™s train an LSTM model for sentiment analysis using a small dataset. Weâ€™ll:\n",
    "1. Preprocess text (convert to sequences).\n",
    "2. Build an LSTM model.\n",
    "3. Train and evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load and Preprocess Data\n",
    "Weâ€™ll use a toy dataset of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset\n",
    "data = {\n",
    "    \"review\": [\n",
    "        \"I loved the movie it was great\",\n",
    "        \"Terrible film so boring and dull\",\n",
    "        \"Amazing story and fantastic acting\",\n",
    "        \"I hated this movie it was awful\",\n",
    "        \"Really enjoyed the plot and characters\",\n",
    "        \"Worst movie ever do not watch\"\n",
    "    ],\n",
    "    \"sentiment\": [1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 1000  # Vocabulary size\n",
    "max_len = 20      # Max sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"review\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"review\"])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Labels\n",
    "y = df[\"sentiment\"].values\n",
    "\n",
    "print(\"\\nPadded sequences:\\n\", padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whatâ€™s Happening?**\n",
    "- **Tokenizer**: Converts words to integers (e.g., \"movie\" â†’ 5).\n",
    "- **Padding**: Ensures all sequences are the same length by adding zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Build and Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Embedding(max_words, 16, input_length=max_len),  # Convert words to dense vectors\n",
    "    LSTM(32, return_sequences=False),               # LSTM layer\n",
    "    Dense(1, activation=\"sigmoid\")                  # Output: probability of positive sentiment\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# Train (small dataset, so few epochs)\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Test a New Review\n",
    "**Your Turn**: Predict the sentiment of a new review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test new review\n",
    "new_review = \"This movie was absolutely fantastic\"\n",
    "new_sequence = tokenizer.texts_to_sequences([new_review])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "prediction = model.predict(new_padded)[0][0]\n",
    "print(f\"Review: {new_review}\\nSentiment: {'Positive' if prediction > 0.5 else 'Negative'} (Probability: {prediction:.2f})\")\n",
    "\n",
    "# Your code here\n",
    "your_review = \"Your review here\"  # Write your own review\n",
    "your_sequence = tokenizer.texts_to_sequences([your_review])\n",
    "your_padded = pad_sequences(your_sequence, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "your_prediction = model.predict(your_padded)[0][0]\n",
    "print(f\"Your review: {your_review}\\nSentiment: {'Positive' if your_prediction > 0.5 else 'Negative'} (Probability: {your_prediction:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How does the LSTM compare to Class 1â€™s BoW model? (Hint: Think about context.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Introduction to Transformers\n",
    "Transformers are the backbone of modern NLP (e.g., BERT, GPT). Unlike RNNs, they:\n",
    "- Use **attention** to focus on important words, regardless of their position.\n",
    "- Process all words at once, not sequentially.\n",
    "- Excel at capturing long-range dependencies.\n",
    "\n",
    "**Analogy**: Instead of reading a book page by page (RNN), transformers skim the whole book and highlight key parts (attention).\n",
    "\n",
    "**Key Concept**:\n",
    "- **Attention**: Weighs which words matter most (e.g., in \"The movie wasnâ€™t great\", \"wasnâ€™t\" affects \"great\").\n",
    "\n",
    "We wonâ€™t build a transformer from scratch (itâ€™s complex!), but weâ€™ll use a pre-trained one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Hands-On: Sentiment Analysis with Transformers\n",
    "Letâ€™s use **Hugging Faceâ€™s pipeline** to perform sentiment analysis with a pre-trained transformer (DistilBERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformer pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Test reviews\n",
    "reviews = [\n",
    "    \"I absolutely loved this movie, it was thrilling!\",\n",
    "    \"This film was a complete waste of time.\"\n",
    "]\n",
    "\n",
    "# Predict sentiment\n",
    "results = sentiment_analyzer(reviews)\n",
    "for review, result in zip(reviews, results):\n",
    "    print(f\"Review: {review}\\nSentiment: {result['label']} (Score: {result['score']:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Test the transformer on two reviews of your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "your_reviews = [\n",
    "    \"Your first review here\",  # Write a review\n",
    "    \"Your second review here\"  # Write another\n",
    "]\n",
    "your_results = sentiment_analyzer(your_reviews)\n",
    "for review, result in zip(your_reviews, your_results):\n",
    "    print(f\"Your review: {review}\\nSentiment: {result['label']} (Score: {result['score']:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How does the transformerâ€™s output compare to the LSTMâ€™s? (Hint: Look at confidence scores.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNNs vs. Transformers\n",
    "- **RNNs**:\n",
    "  - Good for small datasets and simpler tasks.\n",
    "  - Struggle with long sequences.\n",
    "  - Faster to train on small setups.\n",
    "- **Transformers**:\n",
    "  - Handle long contexts better (via attention).\n",
    "  - Require more data and compute.\n",
    "  - Pre-trained models (like BERT) are powerful out of the box.\n",
    "\n",
    "**Discussion**:\n",
    "- When would you use an RNN over a transformer, or vice versa?\n",
    "- How might these models help with your capstone project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wrap-Up\n",
    "Youâ€™ve learned:\n",
    "- How RNNs (like LSTM) process sequences and capture context.\n",
    "- The basics of transformers and their attention mechanism.\n",
    "- How to apply both to sentiment analysis.\n",
    "\n",
    "**Homework**:\n",
    "- Explore Hugging Face: [Hugging Face NLP Course](https://huggingface.co/course).\n",
    "- Start planning your capstone project dataset and task (e.g., text classification, generation).\n",
    "\n",
    "**Deliverables**:\n",
    "- Submit this notebook with completed \"Your Turn\" sections.\n",
    "- Write a short paragraph on one challenge you faced with the LSTM or transformer.\n",
    "\n",
    "**Questions?** Reach out to the instructor or discuss with peers.\n",
    "\n",
    "Great work, and see you in Class 3 for model deployment and ethics! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}