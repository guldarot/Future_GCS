{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 1: Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "## Welcome to Week 10!\n",
    "This notebook introduces **Natural Language Processing (NLP)**, a field of AI that helps computers understand and process human language. By the end, you'll be able to preprocess text data and build a simple sentiment analysis model using the bag-of-words approach.\n",
    "\n",
    "**Objectives**:\n",
    "- Understand what NLP is and its applications.\n",
    "- Learn text preprocessing: tokenization, stop-word removal, stemming/lemmatization.\n",
    "- Explore the bag-of-words model.\n",
    "- Build a sentiment classifier with scikit-learn.\n",
    "\n",
    "**Let's get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is NLP?\n",
    "NLP enables computers to work with text or speech, powering tools like:\n",
    "- Chatbots (e.g., Siri, Grok).\n",
    "- Sentiment analysis (e.g., analyzing movie reviews).\n",
    "- Machine translation (e.g., Google Translate).\n",
    "\n",
    "To make text understandable to machines, we need to **preprocess** it (clean and convert to numbers) and represent it (e.g., as vectors).\n",
    "\n",
    "**Discussion Question**: Can you name other NLP applications you’ve encountered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "Let’s install and import the required libraries. Run the cell below to ensure everything is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (uncomment if needed)\n",
    "# !pip install numpy pandas scikit-learn nltk\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing\n",
    "Raw text is messy (e.g., punctuation, irrelevant words). Preprocessing cleans it up. Key steps:\n",
    "- **Tokenization**: Split text into words (tokens).\n",
    "- **Stop-word removal**: Remove common words (e.g., \"the\", \"is\").\n",
    "- **Stemming/Lemmatization**: Reduce words to their root form (e.g., \"running\" → \"run\").\n",
    "\n",
    "Let’s try each step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tokenization\n",
    "Tokenization breaks text into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text = \"I loved the movie! It was exciting and fun.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text.lower())  # Convert to lowercase\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Tokenize the sentence: \"The cat is sleeping peacefully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "your_text = \"The cat is sleeping peacefully.\"\n",
    "your_tokens = word_tokenize(your_text.lower())\n",
    "print(\"Your tokens:\", your_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stop-Word Removal\n",
    "Stop words (e.g., \"and\", \"the\") don’t add much meaning. Let’s remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from tokens\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"Filtered tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Remove stop words from your tokenized sentence above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "your_filtered_tokens = [word for word in your_tokens if word not in stop_words]\n",
    "print(\"Your filtered tokens:\", your_filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Stemming and Lemmatization\n",
    "- **Stemming**: Chops word endings (e.g., \"running\" → \"run\").\n",
    "- **Lemmatization**: Uses dictionary to get proper root (e.g., \"better\" → \"good\").\n",
    "\n",
    "Let’s compare both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply to filtered tokens\n",
    "stemmed_tokens = [stemmer.stem(word)
    " for word in filtered_tokens]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(\"Stemmed tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Apply stemming and lemmatization to your filtered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "your_stemmed_tokens = [stemmer.stem(word) for word in your_filtered_tokens]\n",
    "your_lemmatized_tokens = [lemmatizer.lemmatize(word) for word in your_filtered_tokens]\n",
    "print(\"Your stemmed tokens:\", your_stemmed_tokens)\n",
    "print(\"Your lemmatized tokens:\", your_lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bag-of-Words Model\n",
    "The **bag-of-words (BoW)** model represents text as a vector of word counts, ignoring order. For example:\n",
    "- Sentence 1: \"I love movies\"\n",
    "- Sentence 2: \"Movies are great\"\n",
    "- Vocabulary: {I, love, movies, are, great}\n",
    "- Vectors: [1, 1, 1, 0, 0] and [0, 0, 1, 1, 1]\n",
    "\n",
    "Let’s use scikit-learn’s `CountVectorizer` to create BoW vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences\n",
    "sentences = [\"I love movies\", \"Movies are great\"]\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "bow_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# View vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW vectors:\\n\", bow_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What are the limitations of BoW? (Hint: Think about word order and meaning.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis with Bag-of-Words\n",
    "Let’s build a sentiment classifier using a small dataset of movie reviews. We’ll:\n",
    "1. Preprocess the text.\n",
    "2. Create BoW vectors.\n",
    "3. Train a Naive Bayes classifier.\n",
    "4. Evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load and Preprocess Data\n",
    "We’ll use a toy dataset for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset\n",
    "data = {\n",
    "    \"review\": [\n",
    "        \"I loved the movie it was great\",\n",
    "        \"Terrible film so boring\",\n",
    "        \"Amazing story and acting\",\n",
    "        \"I hated this movie awful\"\n",
    "    ],\n",
    "    \"sentiment\": [\"positive\", \"negative\", \"positive\", \"negative\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Keep only letters\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"clean_review\"] = df[\"review\"].apply(preprocess_text)\n",
    "print(\"\\nCleaned reviews:\\n\", df[[\"clean_review\", \"sentiment\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create BoW Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to BoW\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"clean_review\"])\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "print(\"BoW vectors:\\n\", X.toarray())\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (small dataset, so we’ll use all for training and test manually)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Test on New Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test new review\n",
    "new_review = \"This movie was fantastic\"\n",
    "clean_new_review = preprocess_text(new_review)\n",
    "new_vector = vectorizer.transform([clean_new_review])\n",
    "prediction = model.predict(new_vector)\n",
    "print(f\"Review: {new_review}\\nPredicted sentiment: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Test the model on a review you write. Try both positive and negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "your_review = \"Your review here\"  # Write your own review\n",
    "clean_your_review = preprocess_text(your_review)\n",
    "your_vector = vectorizer.transform([clean_your_review])\n",
    "your_prediction = model.predict(your_vector)\n",
    "print(f\"Your review: {your_review}\\nPredicted sentiment: {your_prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion\n",
    "- **Challenges**: What issues did you face during preprocessing? (e.g., slang, punctuation)\n",
    "- **Limitations**: Why might BoW fail to capture meaning? (e.g., \"not good\" vs. \"good\")\n",
    "- **Next Steps**: In Class 2, we’ll explore advanced models like RNNs and transformers to handle these limitations.\n",
    "\n",
    "**Homework**:\n",
    "- Read about word embeddings: [Word2Vec Explained](https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model).\n",
    "- Think about a dataset for your capstone project (e.g., reviews, tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "You’ve learned to preprocess text, create a bag-of-words model, and build a sentiment classifier! These skills are the foundation of NLP. Save this notebook and submit it as part of your Class 1 deliverables.\n",
    "\n",
    "**Questions?** Reach out to the instructor or discuss with peers.\n",
    "\n",
    "Happy coding! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}