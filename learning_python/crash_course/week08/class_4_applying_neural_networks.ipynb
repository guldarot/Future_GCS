{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 4: Applying Neural Networks and Mini-Project\n",
    "\n",
    "**Week 8: Introduction to Neural Networks and Deep Learning**\n",
    "\n",
    "Welcome to Class 4 of Week 8! Today, we’ll apply neural networks to a real-world dataset (**MNIST digits**) and compare their performance with a traditional machine learning model (logistic regression). This class culminates in a **mini-project** where you’ll train, evaluate, and analyze both models, synthesizing everything from Classes 1-3.\n",
    "\n",
    "## Objectives\n",
    "- Train a neural network for classification on MNIST digits.\n",
    "- Compare neural network performance with a scikit-learn model.\n",
    "- Complete a mini-project analyzing model accuracy and training time.\n",
    "- Gain intuition about when neural networks outperform traditional methods.\n",
    "\n",
    "## Agenda\n",
    "1. Overview of MNIST dataset.\n",
    "2. Recap: Neural network workflow.\n",
    "3. Demo: Training a neural network and logistic regression on MNIST.\n",
    "4. Mini-Project: Build, compare, and analyze models.\n",
    "\n",
    "Let’s dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of MNIST Dataset\n",
    "\n",
    "The **MNIST dataset** contains 70,000 grayscale images of handwritten digits (0-9), each 28x28 pixels. It’s a standard benchmark for classification:\n",
    "- **Features**: 784 pixel values (28x28 flattened).\n",
    "- **Labels**: 10 classes (digits 0-9).\n",
    "- **Split**: 60,000 training images, 10,000 test images.\n",
    "\n",
    "Unlike Iris (4 features), MNIST is more complex, making it ideal to showcase neural networks’ ability to capture patterns.\n",
    "\n",
    "**Note**: If MNIST feels overwhelming, you can use Iris for the mini-project (instructed in the exercise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recap: Neural Network Workflow\n",
    "\n",
    "From Classes 1-3, we’ve learned:\n",
    "- **Class 1**: Neural networks have neurons, layers, and activation functions (ReLU, softmax).\n",
    "- **Class 2**: Build networks with TensorFlow/Keras (`Sequential`, `Dense`).\n",
    "- **Class 3**: Train via forward/backward propagation, minimizing loss with gradient descent.\n",
    "\n",
    "Today’s workflow:\n",
    "1. Load and preprocess data (normalize pixels, flatten images).\n",
    "2. Build a neural network (e.g., hidden layers with ReLU, softmax output).\n",
    "3. Train and evaluate (accuracy, loss).\n",
    "4. Compare with a scikit-learn model (logistic regression).\n",
    "\n",
    "Let’s see it in action with MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demo: Training Neural Network and Logistic Regression on MNIST\n",
    "\n",
    "We’ll:\n",
    "- Train a neural network with one hidden layer (128 neurons) on MNIST.\n",
    "- Train a logistic regression model as a baseline.\n",
    "- Compare accuracy and training time.\n",
    "\n",
    "Run the code below to see both models in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Flatten images for neural network and logistic regression\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)  # (60000, 784)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)    # (10000, 784)\n",
    "\n",
    "# Print shapes\n",
    "print(f'Training data shape (neural network): {X_train.shape}')\n",
    "print(f'Training data shape (flattened): {X_train_flat.shape}')\n",
    "\n",
    "# Visualize a sample digit\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title(f'Sample Digit: {y_train[0]}')\n",
    "plt.show()\n",
    "\n",
    "# --- Neural Network ---\n",
    "print('Training Neural Network...')\n",
    "nn_start_time = time.time()\n",
    "\n",
    "# Build model\n",
    "nn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),  # Flatten 28x28 images\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Hidden layer\n",
    "    tf.keras.layers.Dense(10, activation='softmax') # Output (10 classes)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "nn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "nn_history = nn_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "nn_test_loss, nn_test_accuracy = nn_model.evaluate(X_test, y_test, verbose=0)\n",
    "nn_time = time.time() - nn_start_time\n",
    "\n",
    "print(f'\\nNeural Network Test Accuracy: {nn_test_accuracy:.4f}')\n",
    "print(f'Neural Network Training Time: {nn_time:.2f} seconds')\n",
    "\n",
    "# --- Logistic Regression ---\n",
    "print('\\nTraining Logistic Regression...')\n",
    "lr_start_time = time.time()\n",
    "\n",
    "# Build and train\n",
    "lr_model = LogisticRegression(max_iter=100, random_state=42)\n",
    "lr_model.fit(X_train_flat, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lr_predictions = lr_model.predict(X_test_flat)\n",
    "lr_test_accuracy = accuracy_score(y_test, lr_predictions)\n",
    "lr_time = time.time() - lr_start_time\n",
    "\n",
    "print(f'Logistic Regression Test Accuracy: {lr_test_accuracy:.4f}')\n",
    "print(f'Logistic Regression Training Time: {lr_time:.2f} seconds')\n",
    "\n",
    "# Plot neural network training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nn_history.history['loss'], label='Training Loss')\n",
    "plt.plot(nn_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Neural Network Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(nn_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(nn_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Neural Network Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **MNIST Preprocessing**:\n",
    "  - Normalized pixels to [0, 1] for faster training.\n",
    "  - Flattened images (28x28 → 784) for logistic regression and neural network input.\n",
    "- **Neural Network**:\n",
    "  - `Flatten`: Converts 28x28 images to 784 inputs.\n",
    "  - Hidden layer: 128 neurons with ReLU.\n",
    "  - Output: 10 neurons with softmax for 10 classes.\n",
    "  - Trained for 5 epochs (fast but effective for demo).\n",
    "- **Logistic Regression**:\n",
    "  - Simple linear model, trained on flattened images.\n",
    "  - `max_iter=100` ensures convergence.\n",
    "- **Results**:\n",
    "  - Neural network typically achieves ~97-98% accuracy.\n",
    "  - Logistic regression gets ~92-93%.\n",
    "  - Compare training times (neural network is slower but more powerful).\n",
    "- **Plots**: Show neural network’s learning progress (loss decreases, accuracy increases).\n",
    "\n",
    "Which model performed better? Why might the neural network outperform logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mini-Project: Build, Compare, and Analyze Models\n",
    "\n",
    "Your task is to complete a **mini-project** by training and comparing models on MNIST (or Iris if preferred). You’ll:\n",
    "1. Build and train a neural network (modify the demo if desired).\n",
    "2. Train a scikit-learn model (e.g., logistic regression, SVM, or random forest).\n",
    "3. Compare accuracy and training time.\n",
    "4. Write a short report (1-2 paragraphs) summarizing your findings.\n",
    "\n",
    "**Options**:\n",
    "- **MNIST**: Use the demo’s preprocessing and modify the neural network (e.g., add a hidden layer, change neurons).\n",
    "- **Iris**: Simpler dataset (from Classes 2-3) if MNIST is too complex.\n",
    "- **Scikit-learn Model**: Try `SVC` (SVM) or `RandomForestClassifier` instead of logistic regression.\n",
    "\n",
    "**Template** (for MNIST): Use the code below and modify at least one aspect (e.g., architecture, epochs, scikit-learn model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your mini-project\n",
    "from sklearn.ensemble import RandomForestClassifier  # Optional alternative\n",
    "\n",
    "# --- Your Neural Network ---\n",
    "print('Training Your Neural Network...')\n",
    "your_nn_start_time = time.time()\n",
    "\n",
    "# Build your model\n",
    "your_nn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "your_nn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "your_nn_history = your_nn_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "your_nn_test_loss, your_nn_test_accuracy = your_nn_model.evaluate(X_test, y_test, verbose=0)\n",
    "your_nn_time = time.time() - your_nn_start_time\n",
    "\n",
    "print(f'\\nYour Neural Network Test Accuracy: {your_nn_test_accuracy:.4f}')\n",
    "print(f'Your Neural Network Training Time: {your_nn_time:.2f} seconds')\n",
    "\n",
    "# --- Your Scikit-learn Model ---\n",
    "print('\\nTraining Your Scikit-learn Model...')\n",
    "your_sk_start_time = time.time()\n",
    "\n",
    "# Build and train (modify as desired, e.g., RandomForestClassifier)\n",
    "your_sk_model = LogisticRegression(max_iter=100, random_state=42)\n",
    "your_sk_model.fit(X_train_flat, y_train)\n",
    "\n",
    "# Evaluate\n",
    "your_sk_predictions = your_sk_model.predict(X_test_flat)\n",
    "your_sk_test_accuracy = accuracy_score(y_test, your_sk_predictions)\n",
    "your_sk_time = time.time() - your_sk_start_time\n",
    "\n",
    "print(f'Your Scikit-learn Test Accuracy: {your_sk_test_accuracy:.4f}')\n",
    "print(f'Your Scikit-learn Training Time: {your_sk_time:.2f} seconds')\n",
    "\n",
    "# Plot your neural network results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(your_nn_history.history['loss'], label='Training Loss')\n",
    "plt.plot(your_nn_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Your Neural Network Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(your_nn_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(your_nn_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Your Neural Network Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-Project Report**:\n",
    "Write 1-2 paragraphs answering:\n",
    "1. What models did you train (neural network architecture, scikit-learn model)?\n",
    "2. What were their accuracies and training times?\n",
    "3. Which performed better, and why do you think so? (Consider model complexity, dataset size.)\n",
    "4. One key insight from your comparison (e.g., neural networks’ strengths, trade-offs).\n",
    "\n",
    "**Questions**:\n",
    "1. What changes did you make to the neural network or scikit-learn model?\n",
    "2. How do your accuracies compare to the demo’s?\n",
    "3. Did training time influence your preference for one model over the other?\n",
    "\n",
    "Write answers and report below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Answers and Report\n",
    "\n",
    "**Questions**:\n",
    "1. **Changes made**: ______\n",
    "2. **Accuracy comparison**: ______\n",
    "3. **Training time influence**: ______\n",
    "\n",
    "**Mini-Project Report**:\n",
    "[Write your 1-2 paragraphs here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "\n",
    "Congratulations! You’ve completed Week 8! Today, you:\n",
    "- Trained a neural network on MNIST for digit classification.\n",
    "- Compared it to a scikit-learn model (logistic regression or other).\n",
    "- Analyzed accuracy, training time, and model strengths in a mini-project.\n",
    "- Applied concepts from neurons to training to real-world tasks.\n",
    "\n",
    "**Homework**:\n",
    "- Submit your mini-project report (PDF or text) via [insert platform, e.g., Canvas].\n",
    "- Include one key insight from your comparison.\n",
    "- Optional: Try another scikit-learn model (e.g., `SVC`) or tweak the neural network (e.g., add dropout: `tf.keras.layers.Dropout(0.2)`).\n",
    "\n",
    "**Reflection**:\n",
    "- Neural networks excel at complex patterns (e.g., images) but take longer to train.\n",
    "- Simple models like logistic regression are faster but less flexible.\n",
    "\n",
    "**Tip**: Ensure libraries are installed:\n",
    "```bash\n",
    "pip install tensorflow scikit-learn numpy matplotlib\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}