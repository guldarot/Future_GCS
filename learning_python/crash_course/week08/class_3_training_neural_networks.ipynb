{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 3: Training Neural Networks\n",
    "\n",
    "**Week 8: Introduction to Neural Networks and Deep Learning**\n",
    "\n",
    "Welcome to Class 3 of Week 8! Today, we’ll dive into **training neural networks**, exploring how they learn from data using forward/backward propagation, gradient descent, and loss functions. We’ll train the Iris classification model from Class 2, evaluate its performance, and visualize the training process.\n",
    "\n",
    "## Objectives\n",
    "- Understand forward and backward propagation.\n",
    "- Learn how gradient descent optimizes weights.\n",
    "- Explore loss functions for classification.\n",
    "- Train and evaluate a neural network using TensorFlow.\n",
    "- Experiment with training hyperparameters (epochs, batch size).\n",
    "\n",
    "## Agenda\n",
    "1. How neural networks learn: Forward/backward propagation.\n",
    "2. Gradient descent and loss functions.\n",
    "3. Training the Iris model (demo).\n",
    "4. Exercise: Train and tune the model.\n",
    "\n",
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How Neural Networks Learn: Forward/Backward Propagation\n",
    "\n",
    "Training a neural network involves adjusting **weights** and **biases** to minimize errors in predictions. This happens in two steps:\n",
    "\n",
    "- **Forward Propagation**:\n",
    "  - Inputs pass through layers (weighted sums, activation functions) to produce predictions.\n",
    "  - Example: For Iris, 4 features → hidden layer (ReLU) → output layer (softmax) → probabilities for 3 classes.\n",
    "  - The **loss function** measures how far predictions are from true labels.\n",
    "\n",
    "- **Backward Propagation** (Backpropagation):\n",
    "  - Computes **gradients** of the loss with respect to weights/biases using the chain rule.\n",
    "  - Gradients indicate how to adjust parameters to reduce loss.\n",
    "\n",
    "**Analogy**: Forward propagation is like guessing an answer; backpropagation is like learning from the mistake to guess better next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent and Loss Functions\n",
    "\n",
    "**Gradient Descent** updates weights/biases in the direction that reduces the loss:\n",
    "- Formula: `weight = weight - learning_rate * gradient`\n",
    "- **Learning Rate**: Controls step size (e.g., 0.001). Too big → overshoots; too small → slow learning.\n",
    "- **Optimizer**: Algorithms like **Adam** (used in our model) improve on basic gradient descent.\n",
    "\n",
    "**Loss Functions** measure prediction error:\n",
    "- **Sparse Categorical Crossentropy**: Used for multi-class classification (like Iris).\n",
    "  - Compares predicted probabilities (softmax outputs) to true labels.\n",
    "- Others: Mean squared error (regression), binary crossentropy (two classes).\n",
    "\n",
    "**Training Process**:\n",
    "- **Epoch**: One pass through the entire training dataset.\n",
    "- **Batch Size**: Number of samples processed before updating weights (e.g., 32).\n",
    "- Goal: Minimize loss while improving accuracy.\n",
    "\n",
    "Let’s train our Iris model to see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Iris Model (Demo)\n",
    "\n",
    "We’ll use the neural network from Class 2 (4 inputs → 10 hidden neurons → 3 outputs) and train it on the Iris dataset. We’ll:\n",
    "- Load and preprocess the data.\n",
    "- Build the model.\n",
    "- Train it for 50 epochs.\n",
    "- Evaluate on the test set.\n",
    "- Plot training loss and accuracy.\n",
    "\n",
    "Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'\\nTest Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Data Prep**: Standardized features (like Class 2) for faster training.\n",
    "- **Model**: Same 4-10-3 architecture (4 inputs, 10 hidden, 3 outputs).\n",
    "- **Training**:\n",
    "  - `epochs=50`: 50 passes through the data.\n",
    "  - `batch_size=32`: Updates weights after every 32 samples.\n",
    "  - `validation_split=0.2`: Uses 20% of training data to monitor performance.\n",
    "- **Evaluation**: Test loss/accuracy shows how well the model generalizes.\n",
    "- **Plots**:\n",
    "  - **Loss**: Should decrease as the model learns.\n",
    "  - **Accuracy**: Should increase, ideally close to 1.0 for Iris.\n",
    "  - **Validation**: Tracks overfitting (if val_loss rises while loss drops).\n",
    "\n",
    "What do you notice about the trends? Are training and validation metrics similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercise: Train and Tune the Model\n",
    "\n",
    "Your turn! Train a new model by modifying the training process or architecture. Try one or more of:\n",
    "- Change the number of **epochs** (e.g., 100 instead of 50).\n",
    "- Adjust the **batch size** (e.g., 16 or 64).\n",
    "- Modify the model (e.g., add a hidden layer with 8 neurons).\n",
    "- Change the **optimizer** (e.g., `sgd` instead of `adam`).\n",
    "\n",
    "**Task**:\n",
    "1. Copy the code below and make at least one change.\n",
    "2. Train the model and evaluate it.\n",
    "3. Plot the loss and accuracy.\n",
    "4. Answer the questions below.\n",
    "\n",
    "Use the template to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your model\n",
    "your_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile your model\n",
    "your_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train your model\n",
    "your_history = your_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "your_test_loss, your_test_accuracy = your_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'\\nYour Test Loss: {your_test_loss:.4f}')\n",
    "print(f'Your Test Accuracy: {your_test_accuracy:.4f}')\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(your_history.history['loss'], label='Training Loss')\n",
    "plt.plot(your_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Your Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(your_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(your_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Your Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "1. What changes did you make (e.g., epochs, batch size, model)?\n",
    "2. What’s your final test accuracy? How does it compare to the demo’s?\n",
    "3. Look at the plots. Is the model overfitting? (Hint: Check if validation loss rises while training loss drops.)\n",
    "\n",
    "Write your answers below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Answers\n",
    "\n",
    "1. **Changes made**: ______\n",
    "2. **Test accuracy and comparison**: ______\n",
    "3. **Overfitting observation**: ______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "\n",
    "Great work! Today, you:\n",
    "- Learned how forward/backward propagation works.\n",
    "- Understood gradient descent and loss functions.\n",
    "- Trained a neural network on Iris and evaluated it.\n",
    "- Visualized training progress and tuned hyperparameters.\n",
    "\n",
    "**Homework**:\n",
    "- Try training on a new dataset, like `load_digits` from scikit-learn (8x8 digit images).\n",
    "  - Hint: Adjust `input_shape` to `(64,)` and output to `Dense(10, activation='softmax')` for 10 classes.\n",
    "- Experiment with more epochs or a different learning rate (e.g., `optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)`).\n",
    "- Optional: Read about optimizers in [TensorFlow’s guide](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n",
    "\n",
    "**Next Class**: We’ll apply neural networks to a larger dataset (MNIST) and compare them to scikit-learn models for our mini-project!\n",
    "\n",
    "**Tip**: Ensure TensorFlow is working:\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}