{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 1: Introduction to Neural Networks\n",
    "\n",
    "**Week 8: Introduction to Neural Networks and Deep Learning**\n",
    "\n",
    "Welcome to the first class of Week 8! Today, we’ll explore the basics of **neural networks**, the foundation of deep learning. By the end, you’ll understand what neural networks are, their key components (neurons, layers, activation functions), and how they process information.\n",
    "\n",
    "## Objectives\n",
    "- Learn what neural networks are and how they’re inspired by the human brain.\n",
    "- Understand the structure: neurons, layers (input, hidden, output).\n",
    "- Explore activation functions and their role in neural networks.\n",
    "- Compute a neuron’s output manually to build intuition.\n",
    "\n",
    "## Agenda\n",
    "1. What are neural networks?\n",
    "2. Components: Neurons, layers, weights, biases.\n",
    "3. Activation functions (sigmoid, ReLU, softmax).\n",
    "4. Demo: Visualizing a neural network.\n",
    "5. Exercise: Compute a neuron’s output.\n",
    "\n",
    "Let’s dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Are Neural Networks?\n",
    "\n",
    "Neural networks are computational models inspired by the human brain. They’re used in AI to solve tasks like image recognition, language processing, and classification.\n",
    "\n",
    "- **Analogy**: Think of a neural network as a team of workers (neurons) passing information through layers, each refining the input to produce a final decision (e.g., “Is this a cat or dog?”).\n",
    "- **Structure**: Neural networks have:\n",
    "  - **Input Layer**: Takes raw data (e.g., pixel values of an image).\n",
    "  - **Hidden Layers**: Process data through calculations.\n",
    "  - **Output Layer**: Produces the final result (e.g., class probabilities).\n",
    "\n",
    "Each neuron in a layer is connected to neurons in the next layer via **weights** and **biases**, which are adjusted during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Components of Neural Networks\n",
    "\n",
    "Let’s break down the building blocks:\n",
    "\n",
    "- **Neuron**: A computational unit that takes inputs, applies weights, adds a bias, and produces an output.\n",
    "  - Formula: `output = activation_function(∑(input * weight) + bias)`\n",
    "- **Weights**: Numbers that scale the importance of inputs.\n",
    "- **Biases**: Constants that shift the output, adding flexibility.\n",
    "- **Layers**:\n",
    "  - **Input Layer**: One neuron per feature (e.g., 4 neurons for Iris dataset’s 4 features).\n",
    "  - **Hidden Layers**: Where the “magic” happens—learn patterns.\n",
    "  - **Output Layer**: Matches the task (e.g., 3 neurons for 3 Iris classes).\n",
    "\n",
    "Below, we’ll visualize a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to draw a simple neural network\n",
    "def plot_neural_network():\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Define layers: [input, hidden, output]\n",
    "    layers = [2, 3, 1]  # 2 input neurons, 3 hidden, 1 output\n",
    "    layer_names = ['Input', 'Hidden', 'Output']\n",
    "    \n",
    "    # Plot neurons\n",
    "    for i, layer_size in enumerate(layers):\n",
    "        for j in range(layer_size):\n",
    "            # Draw neuron (circle)\n",
    "            circle = plt.Circle((i * 2, -j), 0.3, color='skyblue', ec='black')\n",
    "            ax.add_patch(circle)\n",
    "            # Label neuron\n",
    "            if i == 0:\n",
    "                ax.text(i * 2 - 0.5, -j, f'x{j+1}', fontsize=12)\n",
    "            elif i == len(layers)-1:\n",
    "                ax.text(i * 2 + 0.5, -j, 'y', fontsize=12)\n",
    "            else:\n",
    "                ax.text(i * 2 - 0.1, -j, f'h{j+1}', fontsize=12)\n",
    "        # Label layer\n",
    "        ax.text(i * 2, layer_size / 2 + 0.5, layer_names[i], fontsize=14)\n",
    "    \n",
    "    # Draw connections (weights)\n",
    "    for i in range(len(layers)-1):\n",
    "        for j in range(layers[i]):\n",
    "            for k in range(layers[i+1]):\n",
    "                ax.plot([i * 2 + 0.3, (i + 1) * 2 - 0.3], [-j, -k], 'k-', alpha=0.5)\n",
    "    \n",
    "    # Set up plot\n",
    "    ax.set_xlim(-1, len(layers) * 2)\n",
    "    ax.set_ylim(-max(layers), max(layers) / 2 + 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    plt.title('Simple Neural Network (2-3-1)', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the network\n",
    "plot_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: This diagram shows a neural network with:\n",
    "- 2 input neurons (e.g., two features like petal length, width).\n",
    "- 3 hidden neurons (processing the inputs).\n",
    "- 1 output neuron (e.g., predicting a single class).\n",
    "\n",
    "The lines represent **weights**, and each neuron applies a **bias** and **activation function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Functions\n",
    "\n",
    "Activation functions decide whether a neuron “fires” and shape its output. They introduce non-linearity, allowing neural networks to learn complex patterns.\n",
    "\n",
    "Common activation functions:\n",
    "- **Sigmoid**: Squashes output to (0, 1). Used for binary classification.\n",
    "  - Formula: `f(x) = 1 / (1 + e^(-x))`\n",
    "- **ReLU (Rectified Linear Unit)**: Outputs `max(0, x)`. Fast and prevents negative outputs.\n",
    "- **Softmax**: Converts outputs to probabilities (sums to 1). Used in multi-class classification.\n",
    "\n",
    "Let’s visualize sigmoid and ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Generate data\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_relu = relu(x)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y_sigmoid, label='Sigmoid', color='blue')\n",
    "plt.title('Sigmoid Activation')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, y_relu, label='ReLU', color='red')\n",
    "plt.title('ReLU Activation')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Sigmoid**: Smoothly maps any input to (0, 1), great for probabilities.\n",
    "- **ReLU**: Keeps positive values, sets negatives to 0, making training faster.\n",
    "\n",
    "Softmax is typically used in the output layer for multi-class tasks—we’ll see it in Class 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercise: Compute a Neuron’s Output\n",
    "\n",
    "Let’s apply what we’ve learned! Your task is to compute the output of a single neuron manually.\n",
    "\n",
    "**Problem**:\n",
    "- **Inputs**: `[1, 2]`\n",
    "- **Weights**: `[0.5, -0.3]`\n",
    "- **Bias**: `0.1`\n",
    "- **Activation Function**: ReLU\n",
    "\n",
    "**Steps**:\n",
    "1. Compute the weighted sum: `sum = (input1 * weight1) + (input2 * weight2) + bias`\n",
    "2. Apply ReLU: `output = max(0, sum)`\n",
    "\n",
    "Try calculating it by hand, then check your answer with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute neuron output\n",
    "inputs = np.array([1, 2])\n",
    "weights = np.array([0.5, -0.3])\n",
    "bias = 0.1\n",
    "\n",
    "# Step 1: Weighted sum\n",
    "weighted_sum = np.dot(inputs, weights) + bias\n",
    "print(f'Weighted sum: {weighted_sum}')\n",
    "\n",
    "# Step 2: Apply ReLU\n",
    "output = relu(weighted_sum)\n",
    "print(f'Output (after ReLU): {output}')\n",
    "\n",
    "# Your turn: Modify the inputs or weights and recompute!\n",
    "# Example: Change inputs to [0, 1] and weights to [0.2, 0.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task**:\n",
    "1. Verify the output by hand (show your steps in a markdown cell below or on paper).\n",
    "2. Modify the code (e.g., change inputs to `[0, 1]`, weights to `[0.2, 0.4]`) and run it.\n",
    "3. What happens if the weighted sum is negative? Why?\n",
    "\n",
    "Write your answers below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Answers\n",
    "\n",
    "**Hand calculation**:\n",
    "- Weighted sum: ______\n",
    "- ReLU output: ______\n",
    "\n",
    "**Modified code results**:\n",
    "- New inputs: ______\n",
    "- New weights: ______\n",
    "- New output: ______\n",
    "\n",
    "**Negative sum question**:\n",
    "- What happens if the weighted sum is negative? ______\n",
    "- Why? ______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "\n",
    "Great work! Today, you:\n",
    "- Learned what neural networks are and their structure.\n",
    "- Explored neurons, weights, biases, and layers.\n",
    "- Understood activation functions (sigmoid, ReLU).\n",
    "- Computed a neuron’s output manually.\n",
    "\n",
    "**Homework**:\n",
    "- Read the TensorFlow Keras guide: [https://www.tensorflow.org/guide/keras](https://www.tensorflow.org/guide/keras).\n",
    "- Experiment with the code above (try different inputs/weights).\n",
    "- Optional: Play with [TensorFlow Playground](https://playground.tensorflow.org/) to visualize neural networks.\n",
    "\n",
    "Next class, we’ll build a neural network using TensorFlow! Make sure you have TensorFlow installed:\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}